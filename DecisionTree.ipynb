{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "DecisionTree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZuQQMiQeUX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxO69Wl9QeUr"
      },
      "source": [
        "#Reading data from CSV file using pandas dataframe. We have dropped the 7th feature (Var7) from the\n",
        "#data set as it created some unexpected error \n",
        "\n",
        "df = pd.read_csv(\"variance.csv\")\n",
        "df = df.drop(\"File Name\", axis=1)\n",
        "df = df.drop(\"Var7\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liMlRhF2QeU_"
      },
      "source": [
        "df = df.rename(columns = {\"CLASSLABEL\":\"label\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XmlpWcgQeVM"
      },
      "source": [
        "#This function is used for splitting the data into training set (train_df), validation set (val_df) and testing set (test_df)\n",
        "\n",
        "def Data_split(df,test_size, val_size):\n",
        "    if isinstance(test_size,float):\n",
        "        test_size = round(test_size*len(df))\n",
        "        val_size = round(val_size*len(df))\n",
        "\n",
        "    indices = df.index.tolist()\n",
        "    test_indices = random.sample(population=indices, k=test_size)        #Random sampling of the data set\n",
        "    test_df = df.loc[test_indices]\n",
        "    rest_df = df.drop(test_indices)\n",
        "    \n",
        "    indices1 = rest_df.index.tolist()\n",
        "    val_indices = random.sample(population=indices1, k=val_size)         # Random sampling of the remainder of the data set\n",
        "    val_df = rest_df.loc[val_indices]\n",
        "    train_df = rest_df.drop(val_indices)\n",
        "    \n",
        "    return train_df, test_df, val_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y-aYvcIQeVV"
      },
      "source": [
        "random.seed(0)\n",
        "train_df, test_df, val_df = Data_split(df,test_size=20, val_size = 30)  #Partitioning the training, validation and testing data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6e9u47QeVi"
      },
      "source": [
        "data=train_df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBK_otivQeVs"
      },
      "source": [
        "# This function is used to check the purity of node, i.e whether there is one class label present at a node or if more than \n",
        "# one (in our case two class labels, 1 and 0) class labels are present in a particular node.\n",
        "\n",
        "def check_node_purity(data):\n",
        "    \n",
        "    label_column = data[:,-1]\n",
        "    unique_classes = np.unique(label_column)\n",
        "\n",
        "    if len(unique_classes) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN4zP34YQeVz"
      },
      "source": [
        "# This function is used to check how many classes are there in the dataset\n",
        "# the last column of the dataset contains the class-label\n",
        "# In our case, label 1 corresponds to motor image stimulation and label 0 corresponds to visual stimulation\n",
        "def classify_data(data):\n",
        "\n",
        "    label_column = data[:,-1]\n",
        "    unique_classes, counts_unique_classes = np.unique(label_column,return_counts=True)\n",
        "    \n",
        "    index = counts_unique_classes.argmax()\n",
        "    classification = unique_classes[index]\n",
        "\n",
        "    return classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ-tf9hyQeV8"
      },
      "source": [
        "# This function is used to find the mid values of two consecutive values of a particular feature and to consider that\n",
        "# as a potential value to form a question in a node in the tree\n",
        "\n",
        "def get_potential_splits(data):\n",
        "    \n",
        "    potential_splits = {}\n",
        "    _, n_columns = data.shape \n",
        "    for column_index in range(n_columns-1):\n",
        "        potential_splits[column_index] = []\n",
        "        values = data[:,column_index]\n",
        "        unique_values = np.unique(values)\n",
        "\n",
        "        for index in range(len(unique_values)):\n",
        "            if index != 0:\n",
        "                current_value = unique_values[index]\n",
        "                previous_value = unique_values[index-1]\n",
        "                potential_split = (current_value+previous_value)/2\n",
        "                \n",
        "                potential_splits[column_index].append(potential_split)\n",
        "    \n",
        "    return potential_splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BHOPQ_DQeWE"
      },
      "source": [
        "# This function is used to divide the dataset at a node based on a particular yes/no question\n",
        "# The questions are asked in the following form 'Is the feature < a particular split value?'\n",
        "# data_below stores the data that has the considered feature with values less or equal to the split value\n",
        "#data_above stores the data that has the considered feature with values greater than the split value\n",
        "\n",
        "def data_partition_by_question(data, split_column, split_value):\n",
        "    \n",
        "    split_column_values = data[:,split_column]\n",
        "    data_below = data[split_column_values <= split_value]\n",
        "    data_above = data[split_column_values > split_value]\n",
        "    \n",
        "    return data_below, data_above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miCGr_2mQeWM"
      },
      "source": [
        "# This function is used to compute entropy based on the frequency of occurence of the class labels at a node\n",
        "\n",
        "def entropy_calculation(data):\n",
        "    \n",
        "    label_column = data[:,-1]\n",
        "    _, counts = np.unique(label_column, return_counts=True)\n",
        "    \n",
        "    probabilities = counts/counts.sum()\n",
        "    entropy = sum(probabilities*-np.log2(probabilities))\n",
        "       \n",
        "    return entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7AG4lJKQeWW"
      },
      "source": [
        "# This function is used to find the entropy gain at a node\n",
        "\n",
        "def calculate_overall_entropy(data_below, data_above):\n",
        "    \n",
        "    n_data_points = len(data_below)+len(data_above)      #Calculating total number of data points\n",
        "\n",
        "    p_data_below = len(data_below)/n_data_points      #Calculating frequency (probability)\n",
        "    p_data_above = len(data_above)/n_data_points      #Calculating frequency (probability) \n",
        "\n",
        "    overall_entropy = (p_data_below*entropy_calculation(data_below))+(p_data_above*entropy_calculation(data_above)) \n",
        "    \n",
        "    return overall_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMdX3FsQeWf"
      },
      "source": [
        "# This function is used to select the best feature and the corresponding value to form the yes/no questionto be asked\n",
        "# at a node on the basis of enropy gain\n",
        "\n",
        "def determine_best_split(data, potential_splits):\n",
        "    overall_entropy = 999\n",
        "    for column_index in potential_splits:\n",
        "        for value in potential_splits[column_index]:\n",
        "            data_below, data_above = data_partition_by_question(data,split_column=column_index, split_value=value)\n",
        "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
        "        \n",
        "            if current_overall_entropy <= overall_entropy:\n",
        "                overall_entropy = current_overall_entropy\n",
        "                best_split_column = column_index\n",
        "                best_split_column = column_index\n",
        "                best_split_value = value\n",
        "    \n",
        "    return best_split_column, best_split_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8KIf_RQeWq"
      },
      "source": [
        "# This is the main decision tree algorithm. If all the examples at a node are of same label then that class-label is returned.\n",
        "# Otherwise, if there is impurity at a node then this function is called recursively to grow the decision tree\n",
        "\n",
        "\n",
        "def decision_tree_algorithm(df, counter=0):\n",
        "    if counter == 0:\n",
        "        global column_headers             #This part is used to change the data from pandas dataframe to numpy 2D array\n",
        "        column_headers = df.columns\n",
        "        data = df.values\n",
        "    else:\n",
        "        data = df\n",
        "        \n",
        "    \n",
        "    if check_node_purity(data):\n",
        "        classification = classify_data(data)        #calss label is returned for nodes, where all the data are of same class\n",
        "        return classification                       # i.e. the node is not impure and the node is declared as a leaf\n",
        "        \n",
        "    \n",
        "        #recursive part for impure nodes\n",
        "        \n",
        "    else:\n",
        "        counter+=1\n",
        "            \n",
        "        #calling other functions\n",
        "        potential_splits = get_potential_splits(data)\n",
        "        split_column, split_value = determine_best_split(data,potential_splits)\n",
        "        data_below, data_above = data_partition_by_question(data, split_column, split_value)\n",
        "            \n",
        "        # sub-tree create\n",
        "        feature_name = column_headers[split_column]\n",
        "        question = \"{} <= {}\".format(feature_name, split_value)\n",
        "        sub_tree = {question: []}\n",
        "            \n",
        "        yes_answer = decision_tree_algorithm(data_below,counter)\n",
        "        no_answer = decision_tree_algorithm(data_above,counter)\n",
        "            \n",
        "        if yes_answer ==no_answer :\n",
        "            sub_tree = yes_answer\n",
        "        else:\n",
        "            sub_tree[question].append(yes_answer)\n",
        "            sub_tree[question].append(no_answer)\n",
        "        \n",
        "        return sub_tree\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F5_gQ_KQeWy"
      },
      "source": [
        "tree = decision_tree_algorithm(train_df)      # training the decision tree based on the training data set (train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqLExd_fQeW8"
      },
      "source": [
        "# This function is used to get a class label for a new data from the trained decision tree structure\n",
        "\n",
        "def classification(example, tree):\n",
        "    question = list(tree.keys())[0]\n",
        "    feature_name, operator, value = question.split()\n",
        "\n",
        "    if example[feature_name] <= float(value):\n",
        "        answer = tree[question][0]\n",
        "    else:\n",
        "        answer = tree[question][1]\n",
        "    \n",
        "    if not isinstance(answer,dict):\n",
        "        return answer\n",
        "    else:\n",
        "        residual_tree = answer\n",
        "        return classification(example, residual_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTQyDo0tQeXC"
      },
      "source": [
        "# This function calls the 'classification' function on the entire testing set and there by checks the accuracy of the algorithm\n",
        "# Once all the labels are determined from the algorithm we count for how many cases the class labels provided by the algorithm\n",
        "# and the class labels provided by the data set (testing data) are equal. We count such equal examples. If for 'm' cases the two\n",
        "# sets of class labels are same and there are n number of data points in testing set, then accuracy = m/n, ie accuracy is the\n",
        "# mean value of the true counts\n",
        "\n",
        "def calculate_accuracy(df, tree):\n",
        "    \n",
        "    \n",
        "    df[\"classification\"] = df.apply(classification, axis = 1, args=(tree,))\n",
        "    df[\"classification_correct\"] = df.classification == df.label\n",
        "    \n",
        "    accuracy = df.classification_correct.mean()\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XRkOEZIQeXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a28d3bc2-f5b1-4b1c-bc8f-895f4d6e59a1"
      },
      "source": [
        "# Accuracy obtained on the the testing set of the maximum depth decision tree (Overfitted Case)\n",
        "calculate_accuracy(test_df, tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6zJdVb1QeXS"
      },
      "source": [
        "# This function is used to locate the appropriate feature values based on which a particular question is asked at a node\n",
        "\n",
        "def select_feature_df(df, question):\n",
        "    \n",
        "    feature, _, value = question.split()\n",
        "    \n",
        "    df_yes = df[df[feature]<=float(value)]\n",
        "    df_no = df[df[feature]>float(value)]\n",
        "    \n",
        "    return df_yes, df_no"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbgihqNqQeXY"
      },
      "source": [
        "# This function is used for pruning a particular node\n",
        "\n",
        "def pruning_result(tree, train_df, val_df):\n",
        "    \n",
        "    leaf = train_df.label.value_counts().index[0]\n",
        "    errors_leaf = sum(val_df.label != leaf)\n",
        "\n",
        "    val_df[\"classification\"] = val_df.apply(classification, axis = 1, args=(tree,))\n",
        "    make_prediction = val_df[\"classification\"]                                  # Checking the output of the validation dataset\n",
        "\n",
        "    errors_decision_node = sum(val_df.label!=make_prediction)\n",
        "\n",
        "    if errors_leaf <= errors_decision_node:\n",
        "        return leaf\n",
        "    else:\n",
        "        return tree\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP0vYnoGQeXe"
      },
      "source": [
        "# This function is used for pruning the entire decision tree. Pruning is performed as long as the error in the validation set is less than the error in the training set. We consider a node and make it a lef by determining the class by majority in  that node. We compute the accuracy in the validation set and compare it with error obtained by the non pruned tree. If erroris reduced, pruning is performed, else not. Pruning is performed recursively.\n",
        "\n",
        "def post_pruning(tree, train_df, val_df):\n",
        "    \n",
        "    question = list(tree.keys())[0]\n",
        "    yes_answer, no_answer = tree[question]\n",
        "\n",
        "    #basecase\n",
        "    if not isinstance(yes_answer,dict) and not isinstance(no_answer,dict):\n",
        "        return pruning_result(tree, train_df, val_df)\n",
        "        \n",
        "        \n",
        "        #recursion\n",
        "    else:\n",
        "        \n",
        "        train_df_yes, train_df_no = select_feature_df(train_df, question)  \n",
        "        val_df_yes, val_df_no = select_feature_df(val_df, question)\n",
        "        \n",
        "        if isinstance(yes_answer, dict):\n",
        "            tree[question][0] = post_pruning(yes_answer, train_df_yes, val_df_yes)\n",
        "        \n",
        "        if isinstance(no_answer, dict):\n",
        "            tree[question][1] = post_pruning(no_answer, train_df_no, val_df_no)\n",
        "        \n",
        "        return pruning_result(tree, train_df, val_df)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWB4MM13QeXr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d7ef119-1b29-4bb3-aa8b-a19294730b56"
      },
      "source": [
        "post_pruning(tree, train_df, val_df)    # Pruning the decision tree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Var1 <= 5009.133': [0,\n",
              "  {'Var14 <= 28278.936999999998': [0.0,\n",
              "    {'Var14 <= 38198.795999999995': [1.0, 0.0]}]}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbtGVIrQeX0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16e42634-f414-4b1c-ef4e-d128e393d267"
      },
      "source": [
        "# Checking the accuracy on the testing set of the pruned tree based on a validation set\n",
        "\n",
        "calculate_accuracy(test_df, tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9cp_ggKQeX6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnwzDtM1QeYD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C19jzCsQeYJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLslVPUfQeYR"
      },
      "source": [
        ""
      ]
    }
  ]
}